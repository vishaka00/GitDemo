//SparkSession class entry point for SparkSQL
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().appName("Spark SQL").getOrCreate()

// For implicit conversions like converting RDDs to DataFrames
import spark.implicits._

================================================================
#To Create DataFrames

val df = spark.read.json("examples/src/main/resources/people.json")

df.show()

df.printSchema()

df.select("name").show()
df.select($"name", $"age" + 1).show()
df.filter($"age" > 21).show()

================================================================
#Running SQL Queries

df.createOrReplaceTempView("people")

val sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()
spark.sql("SELECT * FROM people where age < 29").show

================================================================
#Creating Datasets
case class Person(name: String, age: Long)

// Encoders are created for case classes
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()


#DataFrames can be converted to a Dataset by providing a class
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()

================================================================
#Inferring the Schema Using Reflection

import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.Encoder

// For implicit conversions from RDDs to DataFrames
import spark.implicits._

val peopleDF = spark.sparkContext.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(attributes => Person(attributes(0), attributes(1).trim.toInt)).toDF()

df.show()

df.printSchema()

// Register the DataFrame as a temporary view

peopleDF.createOrReplaceTempView("people")

SQL statements can be run by using the sql methods provided by Spark
val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")